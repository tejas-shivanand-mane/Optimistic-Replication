===========================================================================
      M/5  E/4
A    3      3
B    1      3
C    2      2

Deriving results in a careful manner is a nice contribution.
——
potentially more flexible than alternatives. Whereas existing work introduced coordination
statically between the methods that *may* conflict, the authors allow doing this
dynamically, only if the methods conflict at runtime.
——
well-written
approach to order concurrent (optimistic) calls is novel and interesting
This decision does not require a distributed agreement because it is stable (Lemma 15 and 16). This is an interesting observation, and to the best of my knowledge this paper is the first to formulate it.


===========================================================================
Review #484A

What is truly missing is an interesting set of examples / applications, together with an in-depth discussion of the relation of the results to CRDTs and such.

Additional issues:

- What is the difference between \emph{background} and \emph{related works}?

- Do the three different implementations provide different guarantees? Or rely on different properties.

- Emphasize that the notion of commutativity is fairly strong, namely, two methods (left or right) commute if they \emph{universally} commute in all instances (depending on arguments and the state in which they are applied).

- A related work not discussed in the paper is by Aspnes and Herlihy (https://dl.acm.org/doi/pdf/10.1145/97444.97701). Although stated in the shared memory model, this paper studies a generic way to implement objects where all operations either commute or overwrite.

> Todo

- It is difficult to figure out what progress guarantees are provided by the implementations.
Especially in the synchronous implementation, do all operations get completed?

- Related: because the eventually synchronous implementations rely on some kind of consensus, this holds only after GST.


===========================================================================
Review #484B

- The solutions presented are very heavy-handed: they require rejecting
operations and retrying them, or first accepting them and then aborting
them. The latter is particularly controversial. Of course, in strongly
consistent databases we are used to transactions being aborted, but why should a
client be fine with this in an eventually consistent system? If a client has to
wait until the system decides whether the operation is committed or aborted
(which requires coordination), isn't this as costly as just providing strong
consistency? The authors don't provide any characterization, either quantitative
or qualitative, about the level of availability provided by their solution,
which would help to address this concern. It's also unclear whether the
additional flexibility of their more dynamic approach is worth the extra
complexity wrt existing solutions (cited in the related work part).

... Beyond this, I am unable to summarize the idea of the last protocol.
- The presentation is suboptimal. Key definitions aren't motivated, and the
presentation of the last protocol is very involved and lacks intuition. I could
more or less understand the first two protocols, but I just got a stack overflow
on the third one.

- The value of the first two protocols is unclear: they assume that the conflict
graphs is acyclic, and I don't know how common this would be for
applications. No application examples are provided. It looks like the authors
meant the first two protocols to be stepping stones for the third protocol, but
they didn't help me much when I got to the latter.

More comments:

S2. You assume that methods don't return a value, only update the state. My
understanding that this setting is useless, since the user can't get a response
to any operation. It's unclear whether adding return values is a simple
extension or would require more invasive changes to the framework.

S2. We say a call c2 permissible-right-commutes (PR-commutes) with another call
c1..., if for every state \sigma: shouldn't your require that \sigma satisfies
integrity? Also, an example would be helpful to illustrate this notion.

S2. "If G_S^M has no cycles, then all calls state commute with one another." -
This looks false to me. If you have even a single edge in G_S^M, this means you
have a pair of non-commuting commands, and executing them in different orders
will yield different states.

S2. You don't explain the construction of the graph G^\box in a reasonable way,
except noting the goal of minimizing the number of cycles. Why do you union G_PR
and the reverse of G_PL and not, for example, G_PL and the reverse of G_PR. Why
do you use this particular combination of commutativity and integrity conflict
relations? You need to motivate the definition of the graph better. An example
would help.

S3. You should say that you assume clocks at processes are perfectly
synchronized - which seems to be your assumption.

S5. You didn't say what the edges of G^\Delta are.

S5. This section is very difficult to follow. I'd suggest you to drop the first
two protocols and explain this protocol properly, since it seems to be your key
contribution. It may also be better to removing the splitting into the "simple
handlers" and the real ones. Having precise pseudocode and a running example
would make it easier to navigate the section. I also think more intuition given
in the text to understand the protocol. Finally, the benefits of the solution
wrt more static alternatives should be characterized more precisely.

The authors seem to have misinterpreted the phrase "there is no page limit" in
the CFP. The CFP states that "the initial 15 pages should contain a clear
presentation of the merits of the paper, including a discussion of the paper’s
importance within the context of prior work and a description of the key
technical and conceptual ideas used to achieve its main claims." The initial 15
pages of the paper end midway through the description of the authors main
result. And even that, only after the authors hacked the standard LIPICS font to
a more compact one. The authors provide all the gory details of the proofs, an
unmotivated definition of G^\Box these proofs rely on and not much in-between.
It's unclear how the paper would look like in camera-ready if it's accepted.


===========================================================================
Review #484C

- This is incremental. wrt. existing literature.
- Some key ideas are not detailed enough and/or formalized.
- The paper lacks motivation.

Some important pieces of information are missing from the text.
Most notably, I would add the following descriptions:  
1a) A formal specification of the stabilization oracle.  
1b) A discussion of the system models that permit to implement the oracle.  
1c) How this oracle is used to stabilize a call.  
1d) A formal definition of WCAS and in particular how it accounts for failures.  
1e) How to implement WCAS in the considered system model using the stabilization oracle.

In my view, this paper also lacks motivation. 
In particular, it would be interesting to answer the following questions:  
2a) What is the interest of considering moverness and/or commutativity in the ordering? Prior works such as the Eventually Serializable Data Service [a] do not make such an assumption. It seems that it is for the purpose of efficiency but no result is given to that respect.  
2b) If the partial synchrony assumption is strong enough, SMR is implementable. In this case, it is unclear why an approach such as EvE (where the result of the optimistic execution is returned to the client) is not more efficient than the last proposed solution.

There are also some minor errors in the text:
On page 5, it is written that in a partially-synchronous setting, processes are aware of the bounds (on the computation and network speeds). 
The standard model is that they exist but no one knows them.
On page 17, the authors write that "by removing the abort-validty" property, there is no need for P.
P is not the weakest failure detector for atomic commitment [c].

I suggest to illustrate the notion of moverness with a basic data type (e.g., a set where add() returns a boolean).

A last question is wrt. the third protocol:  
3a) Why not using the stabilization oracle to compute the same decision everywhere? From what I understand of the stabilization oracle, it is equivalent to a membership service. In that case, a process can simply wait until all the operations prior to a call stabilize before deciding upon it.

_[a] Eventually-serializable data services, Fekete et al., PODC '96_

_[b] All about Eve: execute-verify replication for multi-core servers, Kapritsos et al., OSDI '12_

_[c] The Weakest Failure Detectors to Solve Quittable Consensus and Nonblocking Atomic Commit, Guerraoui et al. SIAM J. '12_

===========================================================================
